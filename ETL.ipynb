{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d345b72a",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "# **Proyecto Final Data Science**\n",
    "## **Extracción - Transformación y Carga de datos ETL**\n",
    "\n",
    "Equipo: 2 - Quantum Insights\n",
    "Integrantes:\n",
    "- Felipe Varela - Product Owner\n",
    "- Freddy Yaquive - Data Scientist\n",
    "- Ivan Martinez - Data Scientist\n",
    "- Sebastian Moya - Data Scientist\n",
    "- Nicolás Lazarte - Scrum Master\n",
    "\n",
    "Cohorte: DSFT01\n",
    "\n",
    "Descripción: Este script toma los datos crudos, los limpia, estandariza IDs\n",
    "y los sube a la Base de Datos en la Nube (Supabase).\n",
    "\n",
    "\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b6462fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "727aada9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de base de datos: \n",
      "e:\\_Cursos\\Tareas_Proyectos_y_Notas\\Soy Henry\\ProyectoFinal\\PF-Quantum_Insights\\databases\n",
      "\n",
      "Products.csv cargado correctamente.\n",
      "------------------------------------------------------------------------------------------\n",
      "Iniciando limpieza de datos...\n",
      "   Tamaño final del catálogo: (2000, 10)\n",
      "   Columnas finales: ['unnamed: 0', 'product_id', 'rating', 'productname', 'brand', 'price', 'image_url', 'category', 'subcategory', 'perfil_texto']\n",
      "Archivo limpio guardado en: \n",
      "e:\\_Cursos\\Tareas_Proyectos_y_Notas\\Soy Henry\\ProyectoFinal\\PF-Quantum_Insights\\databases\\lista_productos_completa.csv\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Iniciando subida a Supabase...\n",
      "¡Carga a la Nube COMPLETADA con éxito!\n",
      "Tabla actualizada: 'productos'\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURACIÓN DE RUTAS ---\n",
    "# Detectamos dinámicamente dónde estamos para ubicar la carpeta 'databases'\n",
    "ruta_actual = os.getcwd()\n",
    "if os.path.exists(os.path.join(ruta_actual, \"databases\")):\n",
    "    ruta_bases = os.path.join(ruta_actual, \"databases\")\n",
    "else:\n",
    "    # Si estamos en una subcarpeta, subimos un nivel\n",
    "    ruta_bases = os.path.join(os.path.dirname(ruta_actual), \"databases\")\n",
    "\n",
    "print(f\"Directorio de base de datos: \\n{ruta_bases}\\n\")\n",
    "\n",
    "# --- 2. EXTRACCIÓN (Lectura de CSVs crudos) ---\n",
    "try:\n",
    "    # Cargamos productos\n",
    "    ruta_prods = os.path.join(ruta_bases, \"products.csv\")\n",
    "    df_products = pd.read_csv(ruta_prods)\n",
    "    print(\"Products.csv cargado correctamente.\")\n",
    "    \n",
    "    # Aquí podrías cargar orders.csv y otros si fuera necesario limpiarlos también\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: No se encontró el archivo. {e}\")\n",
    "\n",
    "# --- 3. TRANSFORMACIÓN (Limpieza y Feature Engineering) ---\n",
    "print(f'---'*30)\n",
    "print(\"Iniciando limpieza de datos...\")\n",
    "\n",
    "# A. Estandarización de Nombres de Columnas\n",
    "# Para evitar el problema de 'ProductId' vs 'product_id', lo forzamos a minúsculas\n",
    "df_products.columns = [c.lower() for c in df_products.columns] \n",
    "# Ahora las columnas son: 'product_id', 'productname', 'brand', etc.\n",
    "\n",
    "# B. Manejo de Nulos\n",
    "# Rellenamos vacíos con texto vacío para evitar errores al concatenar\n",
    "cols_texto = ['productname', 'brand', 'category', 'subcategory']\n",
    "for col in cols_texto:\n",
    "    if col in df_products.columns:\n",
    "        df_products[col] = df_products[col].fillna('')\n",
    "\n",
    "# C. Creación del \"Perfil de Texto\" (Feature para NLP)\n",
    "# Creamos la columna aquí para no tener que hacerlo cada vez que entrenamos\n",
    "def crear_sopa(fila):\n",
    "    # Unimos Marca + Categoría + Subcategoría + Nombre\n",
    "    return f\"{fila.get('brand','')} {fila.get('category','')} {fila.get('subcategory','')} {fila.get('productname','')}\"\n",
    "\n",
    "df_products['perfil_texto'] = df_products.apply(crear_sopa, axis=1)\n",
    "df_products['perfil_texto'] = df_products['perfil_texto'].str.lower().str.strip()\n",
    "\n",
    "print(f\"   Tamaño final del catálogo: {df_products.shape}\")\n",
    "print(f\"   Columnas finales: {df_products.columns.tolist()}\")\n",
    "\n",
    "# --- 4. GUARDADO LOCAL (Para uso de los Pipelines de Entrenamiento) ---\n",
    "ruta_clean = os.path.join(ruta_bases, \"lista_productos_completa.csv\")\n",
    "df_products.to_csv(ruta_clean, index=False)\n",
    "print(f\"Archivo limpio guardado en: \\n{ruta_clean}\\n\")\n",
    "\n",
    "# --- 5. CARGA A LA NUBE (Supabase) ---\n",
    "\n",
    "# Configuración de conexión (Puerto 6543 - Modo Transacción/Túnel)\n",
    "# Nota: Usamos la URL que ya validamos que funciona\n",
    "URI_SUPABASE = \"postgresql://postgres.yolftbsmdognqlwfeaon:ProyectoFinal2025@aws-0-us-west-2.pooler.supabase.com:6543/postgres\"\n",
    "\n",
    "print(f'---'*30)\n",
    "print(\"Iniciando subida a Supabase...\")\n",
    "\n",
    "try:\n",
    "    engine = create_engine(URI_SUPABASE)\n",
    "    \n",
    "    # Subimos el DataFrame a la tabla 'productos'\n",
    "    # if_exists='replace': Borra la tabla vieja y crea una nueva (Ideal para desarrollo/ETL completo)\n",
    "    # chunksize: Sube los datos de a poco para no saturar la red\n",
    "    df_products.to_sql('productos', engine, if_exists='replace', index=False, chunksize=500)\n",
    "    \n",
    "    print(\"¡Carga a la Nube COMPLETADA con éxito!\")\n",
    "    print(\"Tabla actualizada: 'productos'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Hubo un error subiendo a la nube: {e}\")\n",
    "    print(\"(Pero el archivo local sí se generó, así que puedes entrenar modelos)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
