{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3280062b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location=('file:///c:\\\\Users\\\\fredd\\\\Desktop\\\\Soy '\n",
       " 'Henry\\\\ProyectoFinal\\\\PF-Quantum_Insights\\\\modelos_recomendacion/mlruns/571001947788603457'), creation_time=1769455506279, experiment_id='571001947788603457', last_update_time=1769455506279, lifecycle_stage='active', name='Modelos de RecomendaciÃ³n - Comparativa', tags={'mlflow.experimentKind': 'custom_model_development'}>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import mlflow\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "df_events = pd.read_csv(\"../databases/events.csv\")\n",
    "df_order_items = pd.read_csv(\"../databases/order_items.csv\")\n",
    "df_orders = pd.read_csv(\"../databases/orders.csv\")\n",
    "df_products = pd.read_csv(\"../databases/products.csv\")\n",
    "df_reviews = pd.read_csv(\"../databases/reviews.csv\")\n",
    "df_users = pd.read_csv(\"../databases/users.csv\")\n",
    "\n",
    "ruta_actual = os.getcwd()\n",
    "mlflow.set_tracking_uri(f\"file:///{ruta_actual}/mlruns\")\n",
    "mlflow.set_experiment(\"Modelos de RecomendaciÃ³n - Comparativa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff22d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_nlp(df):\n",
    "    # Solo nos interesa limpiar la columna del nombre\n",
    "    # No concatenamos ni marca ni categorÃ­a para evitar sesgos\n",
    "    df['perfil_texto'] = df['ProductName'].fillna('').astype(str).str.lower().str.strip()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2db07569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendar_nlp(product_id, df_sim, top_n=3):\n",
    "    # ValidaciÃ³n bÃ¡sica: si no existe el ID, retorna lista vacÃ­a\n",
    "    if product_id not in df_sim.index:\n",
    "        return []\n",
    "    \n",
    "    # Ordenamos por similitud descendente y eliminamos el propio producto\n",
    "    similares = df_sim[product_id].sort_values(ascending=False).drop(product_id, errors='ignore')\n",
    "    \n",
    "    # Retornamos solo los Ã­ndices (IDs) de los top_n\n",
    "    return similares.head(top_n).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6662158a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P000002', 'P000573', 'P000190']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_productos = preparar_nlp(pd.read_csv(\"../databases/products.csv\"))\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df_productos['perfil_texto'])\n",
    "cos_sim = cosine_similarity(tfidf_matrix)\n",
    "df_sim_nlp = pd.DataFrame(cos_sim, index=df_productos['product_id'], columns=df_productos['product_id'])\n",
    "\n",
    "\n",
    "recomendaciones = recomendar_nlp(\"P000001\", df_sim_nlp)\n",
    "print(recomendaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7965317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModeloNLP:\n",
    "#     def __init__(self, df_sim):\n",
    "#         self.df_sim = df_sim\n",
    "\n",
    "#     def recomendar(self, product_id, top_n=3):\n",
    "#         if product_id not in self.df_sim.index:\n",
    "#             return []\n",
    "\n",
    "#         similares = (\n",
    "#             self.df_sim[product_id]\n",
    "#             .sort_values(ascending=False)\n",
    "#             .drop(product_id, errors='ignore')\n",
    "#         )\n",
    "\n",
    "#         return similares.head(top_n).index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04b9c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruta_actual = os.getcwd()\n",
    "\n",
    "# # Identificar la carpeta correcta donde se guardarÃ¡n los modelos\n",
    "# if os.path.exists(os.path.join(ruta_actual, \"modelos_entrenados\")):\n",
    "#     ruta_modelos = os.path.join(ruta_actual, \"modelos_entrenados\")\n",
    "# else:\n",
    "#     # Buscar la carpeta subiendo un nivel si no estÃ¡ en el actual\n",
    "#     ruta_modelos = os.path.join(os.path.dirname(ruta_actual), \"modelos_entrenados\")\n",
    "\n",
    "# # Crear la carpeta automÃ¡ticamente si no existe\n",
    "# if not os.path.exists(ruta_modelos):\n",
    "#     os.makedirs(ruta_modelos)\n",
    "\n",
    "# # Construir la ruta completa con el nombre del archivo final\n",
    "# archivo_salida = os.path.join(ruta_modelos, \"modelo_recomendacion_nlp.pkl\")\n",
    "\n",
    "# modelo_nlp = ModeloNLP(df_sim_nlp)\n",
    "\n",
    "# archivo_salida = os.path.join(ruta_modelos, \"modelo_recomendacion_nlp.pkl\")\n",
    "\n",
    "# with open(archivo_salida, \"wb\") as f:\n",
    "#     pickle.dump(modelo_nlp, f)\n",
    "\n",
    "# print(f\"ðŸ’¾ Modelo NLP guardado en: {archivo_salida}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18089381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelo con una muestra aleatoria de 1000 productos\n",
      "\n",
      "==================================================\n",
      "Evaluacion de cobertura y confianza (Muestra: 1000)\n",
      "==================================================\n",
      "CONFIANZA (Similitud Promedio): 0.5626\n",
      "El modelo encuentra similitudes fuertes (textos muy parecidos).\n",
      "--------------------------------------------------\n",
      "COBERTURA DE CATALOGO: 73.75% (1475/2000 productos)\n",
      "Cobertura moderada.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def evaluar_confianza_cobertura(df_sim, df_todos_productos, k=3, n_muestras=1000):\n",
    "    ids_disponibles = df_sim.index.tolist()\n",
    "    total_catalogo = len(df_todos_productos)\n",
    "    if n_muestras > len(ids_disponibles):\n",
    "        muestras = ids_disponibles\n",
    "        print(f\"Muestra mayor al catalogo. Evaluando TODO el catÃ¡logo ({len(muestras)} items)\")\n",
    "    else:\n",
    "        muestras = np.random.choice(ids_disponibles, n_muestras, replace=False)\n",
    "        print(f\"Evaluando modelo con una muestra aleatoria de {len(muestras)} productos\")\n",
    "\n",
    "    scores_acumulados = []\n",
    "    items_recomendados_unicos = set()\n",
    "    for prod_id in muestras:\n",
    "        sim_series = df_sim.loc[prod_id]\n",
    "        top_recs = sim_series.drop(prod_id, errors='ignore').nlargest(k)\n",
    "        scores_acumulados.extend(top_recs.values)\n",
    "        items_recomendados_unicos.update(top_recs.index)\n",
    "        \n",
    "    # --- CALCULOS FINALES ---\n",
    "    \n",
    "    # 1. Confianza (Similitud Promedio)\n",
    "    confianza_promedio = np.mean(scores_acumulados) if scores_acumulados else 0\n",
    "    \n",
    "    # 2. Cobertura\n",
    "    cobertura = len(items_recomendados_unicos) / total_catalogo\n",
    "    \n",
    "    # --- REPORTE ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Evaluacion de cobertura y confianza (Muestra: {len(muestras)})\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Reporte de Confianza\n",
    "    print(f\"CONFIANZA (Similitud Promedio): {confianza_promedio:.4f}\")\n",
    "    if confianza_promedio > 0.5:\n",
    "        print(\"El modelo encuentra similitudes fuertes (textos muy parecidos).\")\n",
    "    else:\n",
    "        print(\"Las similitudes son bajas. El modelo podrÃ­a estar recomendando cosas poco relacionadas.\")\n",
    "        \n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Reporte de Cobertura\n",
    "    print(f\"COBERTURA DE CATALOGO: {cobertura:.2%} ({len(items_recomendados_unicos)}/{total_catalogo} productos)\")\n",
    "    if cobertura < 0.10:\n",
    "        print(\"Advertencia, siempre se recomienda los mismos pocos productos.\")\n",
    "    elif cobertura > 0.80:\n",
    "        print(\"   âœ… Excelente diversidad. Tu modelo explora casi todo el catÃ¡logo.\")\n",
    "    else:\n",
    "        print(\"Cobertura moderada.\")\n",
    "        \n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return confianza_promedio, cobertura\n",
    "confianza, cobertura = evaluar_confianza_cobertura(df_sim_nlp, df_productos, k=3, n_muestras=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1602af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ“Š REPORTE FINAL NLP (K=5)\n",
      "==================================================\n",
      "Metrica         | Valor     \n",
      "--------------------------------------------------\n",
      "Precision       | 0.0056\n",
      "Recall          | 0.0137\n",
      "F1-Score        | 0.0079\n",
      "--------------------------------------------------\n",
      "MRR             | 0.0108\n",
      "MAP             | 0.0054\n",
      "NDCG            | 0.0091\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def compute_ap(predicciones, objetivos, k):\n",
    "    \"\"\"CÃ¡lculo de Average Precision (AP) para una consulta\"\"\"\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicciones):\n",
    "        if p in objetivos:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(objetivos), k)\n",
    "\n",
    "def compute_ndcg(predicciones, objetivos, k):\n",
    "    \"\"\"CÃ¡lculo de NDCG - quÃ© tan cerca se estÃ¡ de un ranking ideal\"\"\"\n",
    "    dcg = 0.0\n",
    "    idcg = 0.0\n",
    "    for i, p in enumerate(predicciones):\n",
    "        if p in objetivos:\n",
    "            dcg += 1.0 / np.log2(i + 2)\n",
    "    num_posibles = min(len(objetivos), k)\n",
    "    for i in range(num_posibles):\n",
    "        idcg += 1.0 / np.log2(i + 2)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def evaluar_nlp_baskets(df_test, df_sim_nlp, k=3):\n",
    "    ordenes_test = df_test.groupby('order_id')['product_id'].apply(list)\n",
    "    \n",
    "    # diccionario de metricas\n",
    "    metrics = {\n",
    "        'Precision': [], 'Recall': [], 'F1': [],\n",
    "        'MRR': [], 'MAP': [], 'NDCG': []\n",
    "    }\n",
    "\n",
    "    for items in ordenes_test:\n",
    "        if len(items) < 2: continue\n",
    "        for i in range(len(items)):\n",
    "            semilla = items[i]\n",
    "            objetivos = set(items[:i] + items[i+1:])\n",
    "            predicciones = recomendar_nlp(semilla, df_sim_nlp, top_n=k)\n",
    "            \n",
    "            if not predicciones: continue\n",
    "\n",
    "            # Calculos de metricas\n",
    "            aciertos = len(set(predicciones) & objetivos)\n",
    "            \n",
    "            # Precision & Recall\n",
    "            prec = aciertos / k\n",
    "            rec = aciertos / len(objetivos)\n",
    "            metrics['Precision'].append(prec)\n",
    "            metrics['Recall'].append(rec)\n",
    "            \n",
    "            # F1-Score\n",
    "            if (prec + rec) > 0:\n",
    "                f1 = 2 * (prec * rec) / (prec + rec)\n",
    "            else:\n",
    "                f1 = 0.0\n",
    "            metrics['F1'].append(f1)\n",
    "            \n",
    "            # MRR\n",
    "            rank_score = 0\n",
    "            for rank, p_id in enumerate(predicciones, 1):\n",
    "                if p_id in objetivos:\n",
    "                    rank_score = 1 / rank\n",
    "                    break\n",
    "            metrics['MRR'].append(rank_score)\n",
    "            \n",
    "            # MAP & NDCG\n",
    "            metrics['MAP'].append(compute_ap(predicciones, objetivos, k))\n",
    "            metrics['NDCG'].append(compute_ndcg(predicciones, objetivos, k))\n",
    "\n",
    "    # --- REPORTE FINAL ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"ðŸ“Š REPORTE FINAL NLP (K={k})\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Metrica':<15} | {'Valor':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    # Promediamos todos los scores acumulados\n",
    "    print(f\"{'Precision':<15} | {np.mean(metrics['Precision']):.4f}\")\n",
    "    print(f\"{'Recall':<15} | {np.mean(metrics['Recall']):.4f}\")\n",
    "    print(f\"{'F1-Score':<15} | {np.mean(metrics['F1']):.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'MRR':<15} | {np.mean(metrics['MRR']):.4f}\")\n",
    "    print(f\"{'MAP':<15} | {np.mean(metrics['MAP']):.4f}\")\n",
    "    print(f\"{'NDCG':<15} | {np.mean(metrics['NDCG']):.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    name = f\"NLP_K{k}\"\n",
    "    with mlflow.start_run(run_name=name):\n",
    "\n",
    "        # --- ParÃ¡metros ---\n",
    "        mlflow.log_param(\"modelo\", \"NLP\")\n",
    "        mlflow.log_param(\"k\", k)\n",
    "\n",
    "        # --- MÃ©tricas ---\n",
    "        mlflow.log_metric(\"precision\", np.mean(metrics['Precision']))\n",
    "        mlflow.log_metric(\"recall\", np.mean(metrics['Recall']))\n",
    "        mlflow.log_metric(\"f1\", np.mean(metrics['F1']))\n",
    "        mlflow.log_metric(\"mrr\", np.mean(metrics['MRR']))\n",
    "        mlflow.log_metric(\"map\", np.mean(metrics['MAP']))\n",
    "        mlflow.log_metric(\"ndcg\", np.mean(metrics['NDCG']))\n",
    "    return metrics\n",
    "\n",
    "resultados_nlp = evaluar_nlp_baskets(test_data, df_sim_nlp, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbda538",
   "metadata": {},
   "source": [
    "mlflow ui --backend-store-uri ./mlruns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
