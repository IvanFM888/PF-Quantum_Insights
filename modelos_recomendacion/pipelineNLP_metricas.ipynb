{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3280062b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location=('file:///c:\\\\Users\\\\fredd\\\\Desktop\\\\Soy '\n",
       " 'Henry\\\\ProyectoFinal\\\\PF-Quantum_Insights\\\\modelos_recomendacion/mlruns/571001947788603457'), creation_time=1769455506279, experiment_id='571001947788603457', last_update_time=1769455506279, lifecycle_stage='active', name='Modelos de RecomendaciÃ³n - Comparativa', tags={'mlflow.experimentKind': 'custom_model_development'}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import mlflow\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "df_events = pd.read_csv(\"../databases/events.csv\")\n",
    "df_order_items = pd.read_csv(\"../databases/order_items.csv\")\n",
    "df_orders = pd.read_csv(\"../databases/orders.csv\")\n",
    "df_products = pd.read_csv(\"../databases/products.csv\")\n",
    "df_reviews = pd.read_csv(\"../databases/reviews.csv\")\n",
    "df_users = pd.read_csv(\"../databases/users.csv\")\n",
    "\n",
    "ruta_actual = os.getcwd()\n",
    "mlflow.set_tracking_uri(f\"file:///{ruta_actual}/mlruns\")\n",
    "mlflow.set_experiment(\"Modelos de RecomendaciÃ³n - Comparativa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff22d043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Matriz NLP creada: (2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "def preparar_nlp(df):\n",
    "    columnas_clave = ['ProductName', 'Brand', 'Category', 'SubCategory']\n",
    "    for col in columnas_clave:\n",
    "        df[col] = df[col].fillna('')\n",
    "    \n",
    "    # Creamos el perfil de texto de forma eficiente\n",
    "    df['perfil_texto'] = (df['Brand'] + \" \" + df['Category'] + \" \" + \n",
    "                          df['SubCategory'] + \" \" + df['ProductName']).str.lower().str.strip()\n",
    "    return df\n",
    "\n",
    "df_productos = preparar_nlp(pd.read_csv(\"../databases/products.csv\"))\n",
    "\n",
    "# VectorizaciÃ³n: Convertimos texto a nÃºmeros\n",
    "tfidf = TfidfVectorizer(stop_words='english') \n",
    "tfidf_matrix = tfidf.fit_transform(df_productos['perfil_texto'])\n",
    "\n",
    "# Matriz de similitud de Coseno\n",
    "cos_sim = cosine_similarity(tfidf_matrix)\n",
    "df_sim_nlp = pd.DataFrame(cos_sim, index=df_productos['product_id'], columns=df_productos['product_id'])\n",
    "\n",
    "print(f\"âœ… Matriz NLP creada: {df_sim_nlp.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcbb03cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DivisiÃ³n de datos finalizada:\n",
      "   - Ã“rdenes para entrenamiento: 16000\n",
      "   - Ã“rdenes para prueba (test): 4000\n",
      "   - Total de interacciones en test: 12890\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Identificamos todas las Ã³rdenes Ãºnicas\n",
    "ordenes_unicas = df_order_items['order_id'].unique()\n",
    "\n",
    "# 2. Dividimos las Ã³rdenes: 80% para entrenamiento y 20% para prueba\n",
    "# Usamos random_state=42 para que siempre obtengas los mismos resultados\n",
    "train_ids, test_ids = train_test_split(ordenes_unicas, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Creamos los DataFrames filtrando por esos IDs\n",
    "# 'train_data' lo usarÃ­as si quisieras hacer un modelo hÃ­brido mÃ¡s adelante\n",
    "# 'test_data' es el que usaremos en la funciÃ³n 'evaluar_nlp_baskets'\n",
    "train_data = df_order_items[df_order_items['order_id'].isin(train_ids)]\n",
    "test_data = df_order_items[df_order_items['order_id'].isin(test_ids)]\n",
    "\n",
    "print(\"âœ… DivisiÃ³n de datos finalizada:\")\n",
    "print(f\"   - Ã“rdenes para entrenamiento: {len(train_ids)}\")\n",
    "print(f\"   - Ã“rdenes para prueba (test): {len(test_ids)}\")\n",
    "print(f\"   - Total de interacciones en test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db07569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P001100', 'P000132', 'P001099']\n"
     ]
    }
   ],
   "source": [
    "def recomendar_nlp(product_id, df_sim, top_n=3):\n",
    "    if product_id not in df_sim.index:\n",
    "        return []\n",
    "    \n",
    "    # Buscamos los mÃ¡s similares omitiendo el producto mismo\n",
    "    similares = df_sim[product_id].sort_values(ascending=False).drop(product_id, errors='ignore')\n",
    "    return similares.head(top_n).index.tolist()\n",
    "\n",
    "# Prueba rÃ¡pida\n",
    "print(recomendar_nlp(\"P000324\", df_sim_nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b125afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Modelo guardado exitosamente en:\n",
      "c:\\Users\\fredd\\Desktop\\Soy Henry\\ProyectoFinal\\PF-Quantum_Insights\\modelos_entrenados\\modelo_recomendacion_npl.pkl\n"
     ]
    }
   ],
   "source": [
    "ruta_actual = os.getcwd()\n",
    "\n",
    "# Identificar la carpeta correcta donde se guardarÃ¡n los modelos\n",
    "if os.path.exists(os.path.join(ruta_actual, \"modelos_entrenados\")):\n",
    "    ruta_modelos = os.path.join(ruta_actual, \"modelos_entrenados\")\n",
    "else:\n",
    "    # Buscar la carpeta subiendo un nivel si no estÃ¡ en el actual\n",
    "    ruta_modelos = os.path.join(os.path.dirname(ruta_actual), \"modelos_entrenados\")\n",
    "\n",
    "# Crear la carpeta automÃ¡ticamente si no existe\n",
    "if not os.path.exists(ruta_modelos):\n",
    "    os.makedirs(ruta_modelos)\n",
    "\n",
    "# Construir la ruta completa con el nombre del archivo final\n",
    "archivo_salida = os.path.join(ruta_modelos, \"modelo_recomendacion_npl.pkl\")\n",
    "\n",
    "# Guardar la tabla de similitudes en un archivo fÃ­sico\n",
    "with open(archivo_salida, 'wb') as f:\n",
    "    pickle.dump(df_sim_nlp, f) \n",
    "\n",
    "print(f\"ðŸ’¾ Modelo guardado exitosamente en:\\n{archivo_salida}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18089381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelo con una muestra aleatoria de 1000 productos\n",
      "\n",
      "==================================================\n",
      "Evaluacion de cobertura y confianza (Muestra: 1000)\n",
      "==================================================\n",
      "CONFIANZA (Similitud Promedio): 0.6625\n",
      "El modelo encuentra similitudes fuertes (textos muy parecidos).\n",
      "--------------------------------------------------\n",
      "COBERTURA DE CATALOGO: 74.00% (1480/2000 productos)\n",
      "Cobertura moderada.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def evaluar_confianza_cobertura(df_sim, df_todos_productos, k=3, n_muestras=1000):\n",
    "    ids_disponibles = df_sim.index.tolist()\n",
    "    total_catalogo = len(df_todos_productos)\n",
    "    if n_muestras > len(ids_disponibles):\n",
    "        muestras = ids_disponibles\n",
    "        print(f\"Muestra mayor al catalogo. Evaluando TODO el catÃ¡logo ({len(muestras)} items)\")\n",
    "    else:\n",
    "        muestras = np.random.choice(ids_disponibles, n_muestras, replace=False)\n",
    "        print(f\"Evaluando modelo con una muestra aleatoria de {len(muestras)} productos\")\n",
    "\n",
    "    scores_acumulados = []\n",
    "    items_recomendados_unicos = set()\n",
    "    for prod_id in muestras:\n",
    "        sim_series = df_sim.loc[prod_id]\n",
    "        top_recs = sim_series.drop(prod_id, errors='ignore').nlargest(k)\n",
    "        scores_acumulados.extend(top_recs.values)\n",
    "        items_recomendados_unicos.update(top_recs.index)\n",
    "        \n",
    "    # --- CALCULOS FINALES ---\n",
    "    \n",
    "    # 1. Confianza (Similitud Promedio)\n",
    "    confianza_promedio = np.mean(scores_acumulados) if scores_acumulados else 0\n",
    "    \n",
    "    # 2. Cobertura\n",
    "    cobertura = len(items_recomendados_unicos) / total_catalogo\n",
    "    \n",
    "    # --- REPORTE ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Evaluacion de cobertura y confianza (Muestra: {len(muestras)})\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Reporte de Confianza\n",
    "    print(f\"CONFIANZA (Similitud Promedio): {confianza_promedio:.4f}\")\n",
    "    if confianza_promedio > 0.5:\n",
    "        print(\"El modelo encuentra similitudes fuertes (textos muy parecidos).\")\n",
    "    else:\n",
    "        print(\"Las similitudes son bajas. El modelo podrÃ­a estar recomendando cosas poco relacionadas.\")\n",
    "        \n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Reporte de Cobertura\n",
    "    print(f\"COBERTURA DE CATALOGO: {cobertura:.2%} ({len(items_recomendados_unicos)}/{total_catalogo} productos)\")\n",
    "    if cobertura < 0.10:\n",
    "        print(\"Advertencia, siempre se recomienda los mismos pocos productos.\")\n",
    "    elif cobertura > 0.80:\n",
    "        print(\"   âœ… Excelente diversidad. Tu modelo explora casi todo el catÃ¡logo.\")\n",
    "    else:\n",
    "        print(\"Cobertura moderada.\")\n",
    "        \n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return confianza_promedio, cobertura\n",
    "confianza, cobertura = evaluar_confianza_cobertura(df_sim_nlp, df_productos, k=3, n_muestras=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1602af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ“Š REPORTE FINAL NLP (K=5)\n",
      "==================================================\n",
      "Metrica         | Valor     \n",
      "--------------------------------------------------\n",
      "Precision       | 0.0069\n",
      "Recall          | 0.0171\n",
      "F1-Score        | 0.0098\n",
      "--------------------------------------------------\n",
      "MRR             | 0.0126\n",
      "MAP             | 0.0064\n",
      "NDCG            | 0.0110\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def compute_ap(predicciones, objetivos, k):\n",
    "    \"\"\"CÃ¡lculo de Average Precision (AP) para una consulta\"\"\"\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicciones):\n",
    "        if p in objetivos:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(objetivos), k)\n",
    "\n",
    "def compute_ndcg(predicciones, objetivos, k):\n",
    "    \"\"\"CÃ¡lculo de NDCG - quÃ© tan cerca se estÃ¡ de un ranking ideal\"\"\"\n",
    "    dcg = 0.0\n",
    "    idcg = 0.0\n",
    "    for i, p in enumerate(predicciones):\n",
    "        if p in objetivos:\n",
    "            dcg += 1.0 / np.log2(i + 2)\n",
    "    num_posibles = min(len(objetivos), k)\n",
    "    for i in range(num_posibles):\n",
    "        idcg += 1.0 / np.log2(i + 2)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def evaluar_nlp_baskets(df_test, df_sim_nlp, k=3):\n",
    "    ordenes_test = df_test.groupby('order_id')['product_id'].apply(list)\n",
    "    \n",
    "    # diccionario de metricas\n",
    "    metrics = {\n",
    "        'Precision': [], 'Recall': [], 'F1': [],\n",
    "        'MRR': [], 'MAP': [], 'NDCG': []\n",
    "    }\n",
    "\n",
    "    for items in ordenes_test:\n",
    "        if len(items) < 2: continue\n",
    "        for i in range(len(items)):\n",
    "            semilla = items[i]\n",
    "            objetivos = set(items[:i] + items[i+1:])\n",
    "            predicciones = recomendar_nlp(semilla, df_sim_nlp, top_n=k)\n",
    "            \n",
    "            if not predicciones: continue\n",
    "\n",
    "            # Calculos de metricas\n",
    "            aciertos = len(set(predicciones) & objetivos)\n",
    "            \n",
    "            # Precision & Recall\n",
    "            prec = aciertos / k\n",
    "            rec = aciertos / len(objetivos)\n",
    "            metrics['Precision'].append(prec)\n",
    "            metrics['Recall'].append(rec)\n",
    "            \n",
    "            # F1-Score\n",
    "            if (prec + rec) > 0:\n",
    "                f1 = 2 * (prec * rec) / (prec + rec)\n",
    "            else:\n",
    "                f1 = 0.0\n",
    "            metrics['F1'].append(f1)\n",
    "            \n",
    "            # MRR\n",
    "            rank_score = 0\n",
    "            for rank, p_id in enumerate(predicciones, 1):\n",
    "                if p_id in objetivos:\n",
    "                    rank_score = 1 / rank\n",
    "                    break\n",
    "            metrics['MRR'].append(rank_score)\n",
    "            \n",
    "            # MAP & NDCG\n",
    "            metrics['MAP'].append(compute_ap(predicciones, objetivos, k))\n",
    "            metrics['NDCG'].append(compute_ndcg(predicciones, objetivos, k))\n",
    "\n",
    "    # --- REPORTE FINAL ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"ðŸ“Š REPORTE FINAL NLP (K={k})\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Metrica':<15} | {'Valor':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    # Promediamos todos los scores acumulados\n",
    "    print(f\"{'Precision':<15} | {np.mean(metrics['Precision']):.4f}\")\n",
    "    print(f\"{'Recall':<15} | {np.mean(metrics['Recall']):.4f}\")\n",
    "    print(f\"{'F1-Score':<15} | {np.mean(metrics['F1']):.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'MRR':<15} | {np.mean(metrics['MRR']):.4f}\")\n",
    "    print(f\"{'MAP':<15} | {np.mean(metrics['MAP']):.4f}\")\n",
    "    print(f\"{'NDCG':<15} | {np.mean(metrics['NDCG']):.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    name = f\"NLP_K{k}\"\n",
    "    with mlflow.start_run(run_name=name):\n",
    "\n",
    "        # --- ParÃ¡metros ---\n",
    "        mlflow.log_param(\"modelo\", \"NLP\")\n",
    "        mlflow.log_param(\"k\", k)\n",
    "\n",
    "        # --- MÃ©tricas ---\n",
    "        mlflow.log_metric(\"precision\", np.mean(metrics['Precision']))\n",
    "        mlflow.log_metric(\"recall\", np.mean(metrics['Recall']))\n",
    "        mlflow.log_metric(\"f1\", np.mean(metrics['F1']))\n",
    "        mlflow.log_metric(\"mrr\", np.mean(metrics['MRR']))\n",
    "        mlflow.log_metric(\"map\", np.mean(metrics['MAP']))\n",
    "        mlflow.log_metric(\"ndcg\", np.mean(metrics['NDCG']))\n",
    "    return metrics\n",
    "\n",
    "resultados_nlp = evaluar_nlp_baskets(test_data, df_sim_nlp, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbda538",
   "metadata": {},
   "source": [
    "mlflow ui --backend-store-uri ./mlruns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
